{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "975e75df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62d97484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f6a8d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "313b4409",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.row_count = 3\n",
    "        self.column_count = 3\n",
    "        self.action_size = self.row_count*self.column_count\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"TicTacToe\"\n",
    "    \n",
    "    \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        state[row, column] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_move (self, state):\n",
    "        return (state.reshape(-1)==0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        player = state[row, column]\n",
    "        \n",
    "        return (\n",
    "            np.sum(state[row, :]) == player * self.column_count or\n",
    "            np.sum(state[:, column]) == player * self.row_count or\n",
    "            np.sum(np.diag(state)) == player * self.row_count or\n",
    "            np.sum(np.diag(np.flip(state, axis=0))) == player*self.row_count\n",
    "            \n",
    "        )\n",
    "    def get_value_and_terminate(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_move(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        return encoded_state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bbd374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectFour:\n",
    "    def __init__(self):\n",
    "        self.row_count = 3\n",
    "        self.column_count = 3\n",
    "        self.action_size = self.row_count*self.column_count\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"TicTacToe\"\n",
    "    \n",
    "    \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        state[row, column] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_move (self, state):\n",
    "        return (state.reshape(-1)==0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        player = state[row, column]\n",
    "        \n",
    "        return (\n",
    "            np.sum(state[row, :]) == player * self.column_count or\n",
    "            np.sum(state[:, column]) == player * self.row_count or\n",
    "            np.sum(np.diag(state)) == player * self.row_count or\n",
    "            np.sum(np.diag(np.flip(state, axis=0))) == player*self.row_count\n",
    "            \n",
    "        )\n",
    "    def get_value_and_terminate(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_move(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        return encoded_state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6374ffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, game, num_resBlocks, num_hidden, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for i in range (num_resBlocks)]\n",
    "        )\n",
    "        \n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*game.row_count*game.column_count, game.action_size)\n",
    "        )\n",
    "        \n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3*game.row_count*game.column_count, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.startBlock(x)\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value\n",
    "    \n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size = 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5ea539f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 1.  0.  1.]]\n",
      "[[[0. 0. 1.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 0.]\n",
      "  [1. 0. 1.]\n",
      "  [0. 1. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [1. 0. 1.]]]\n",
      "-0.0876760482788086 [1.7636247e-01 1.9745520e-01 1.0887960e-04 1.8109295e-01 1.9909121e-04\n",
      " 1.4348891e-01 1.1479827e-04 3.0115035e-01 2.7353632e-05]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 9 artists>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "tictactoe = TicTacToe()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "state = tictactoe.get_initial_state()\n",
    "state = tictactoe.get_next_state(state, 2, -1)\n",
    "state = tictactoe.get_next_state(state, 4, -1)\n",
    "state = tictactoe.get_next_state(state, 6, 1)\n",
    "state = tictactoe.get_next_state(state, 8, 1)\n",
    "\n",
    "print(state)\n",
    "\n",
    "encoded_state = tictactoe.get_encoded_state(state)\n",
    "\n",
    "print(encoded_state)\n",
    "\n",
    "tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64, device = device)\n",
    "model.load_state_dict(torch.load('model_2_TicTacToe.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis = 1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(value, policy)\n",
    "\n",
    "plt.bar(range(tictactoe.action_size), policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a86b84f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, args, state, parent = None, action_taken = None, prior = 0, visit_count = 0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "        \n",
    "        self.children = []\n",
    "        \n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "    \n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "                \n",
    "        return best_child\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = 1 - (((child.value_sum / child.visit_count) + 1)/2)\n",
    "        return q_value + self.args['C'] * (math.sqrt(self.visit_count)/(child.visit_count + 1)) + child.prior\n",
    "    \n",
    "    def expand(self, policy):\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob>0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player = -1)\n",
    "\n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "                \n",
    "        return child\n",
    "    \n",
    "    \n",
    "    def backpropogate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropogate(value)\n",
    "            \n",
    "            \n",
    "class MCTS:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        root = Node(self.game, self.args, state, visit_count = 1)\n",
    "        \n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(state), device = self.model.device).unsqueeze(0)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis =1).squeeze(0).cpu().numpy()\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "                    * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
    "        \n",
    "        validmoves = self.game.get_valid_move(state)\n",
    "        policy *= validmoves\n",
    "        \n",
    "        policy /= np.sum(policy)\n",
    "        root.expand(policy)\n",
    "        \n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            \n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "            \n",
    "            value, is_terminal = self.game.get_value_and_terminate(node.state, node.action_taken)\n",
    "            value = self.game.get_opponent_value(value)\n",
    "            \n",
    "            if not is_terminal:\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state), device = self.model.device).unsqueeze(0)\n",
    "                )\n",
    "                policy = torch.softmax(policy, axis = 1).squeeze(0).cpu().numpy()\n",
    "                valid_moves = self.game.get_valid_move(node.state)\n",
    "                policy *= valid_moves\n",
    "                policy /= np.sum(policy)\n",
    "                \n",
    "                value = value.item()\n",
    "                \n",
    "                node = node.expand(policy)\n",
    "            \n",
    "            node.backpropogate(value)\n",
    "            \n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        \n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "289abe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, model)\n",
    "        \n",
    "    def selfPlay(self):\n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()\n",
    "        \n",
    "        while True:\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "            temperature_action_probs = action_probs ** (1/self.args['temperature'])\n",
    "            action = np.random.choice(self.game.action_size, p = action_probs)\n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "            value, is_terminal = self.game.get_value_and_terminate(state, action)\n",
    "            \n",
    "            if is_terminal:\n",
    "                returnMemory = []\n",
    "                for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                    returnMemory.append((\n",
    "                        self.game.get_encoded_state(hist_neutral_state),\n",
    "                        hist_action_probs,\n",
    "                        hist_outcome\n",
    "                    ))\n",
    "                return returnMemory\n",
    "            \n",
    "            player = self.game.get_opponent(player)\n",
    "            \n",
    "    \n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIndex in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIndex:min(len(memory)-1,batchIndex+self.args['batch_size'])]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device = self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_target = torch.tensor(value_targets, dtype=torch.float32, device = self.model.device)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_target)\n",
    "            loss = policy_loss + value_loss\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            \n",
    "    \n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n",
    "                memory += self.selfPlay()\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "                \n",
    "            torch.save(self.model.state_dict(), f'model_{iteration}_{self.game}.pt')\n",
    "            torch.save(self.optimizer.state_dict(), f'optimizer_{iteration}_{self.game}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a63228a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebaef6d4fed047b59fc924ec40c6cb39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8ac942294648b7bbf4f3dac074f014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29fa092ba82b4dbb815494c784dc6f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b1157a537242648da77d20717ecc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f8a7b0d029404f9c38a64f2544b715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74bcbf39043d463d81efe28420cfcb8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tictactoe = TicTacToe()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64, device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01, weight_decay=0.0001)\n",
    "\n",
    "args = {\n",
    "    'C':2,\n",
    "    'num_searches':60,\n",
    "    'num_iterations':3,\n",
    "    'num_selfPlay_iterations':500,\n",
    "    'num_epochs':4,\n",
    "    'batch_size':64,\n",
    "    'temperature':1.25,\n",
    "    'dirichlet_epsilon':0.25,\n",
    "    'dirichlet_alpha':0.3\n",
    "}\n",
    "\n",
    "alphazero = AlphaZero(model, optimizer, tictactoe, args)\n",
    "alphazero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77979c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "valid moves :  [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "1: 3\n",
      "[[0. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "[[-1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "valid moves :  [1, 2, 4, 5, 6, 7, 8]\n",
      "1: 5\n",
      "[[-1.  0.  0.]\n",
      " [ 1.  0.  1.]\n",
      " [ 0.  0.  0.]]\n",
      "[[-1.  0.  0.]\n",
      " [ 1. -1.  1.]\n",
      " [ 0.  0.  0.]]\n",
      "valid moves :  [1, 2, 6, 7, 8]\n",
      "1: 8\n",
      "[[-1.  0.  0.]\n",
      " [ 1. -1.  1.]\n",
      " [ 0.  0.  1.]]\n",
      "[[-1.  0. -1.]\n",
      " [ 1. -1.  1.]\n",
      " [ 0.  0.  1.]]\n",
      "valid moves :  [1, 6, 7]\n",
      "1: 1\n",
      "[[-1.  1. -1.]\n",
      " [ 1. -1.  1.]\n",
      " [ 0.  0.  1.]]\n",
      "[[-1.  1. -1.]\n",
      " [ 1. -1.  1.]\n",
      " [-1.  0.  1.]]\n",
      "-1  won\n"
     ]
    }
   ],
   "source": [
    "tictactoe = TicTacToe()\n",
    "player = 1\n",
    "\n",
    "args = {\n",
    "    'C' : 2,\n",
    "    'num_searches' : 1000\n",
    "}\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64)\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(tictactoe, args, model)\n",
    "\n",
    "state = tictactoe.get_initial_state()\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "    \n",
    "    if player == 1:\n",
    "        valid_moves = tictactoe.get_valid_move(state)\n",
    "        print(\"valid moves : \", [i for i in range(tictactoe.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f\"{player}: \"))\n",
    "\n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"action not valid\")\n",
    "            continue\n",
    "    else:\n",
    "        neutral_state = tictactoe.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "        \n",
    "    state = tictactoe.get_next_state(state, action, player)\n",
    "    \n",
    "    value, is_terminal = tictactoe.get_value_and_terminate(state, action)\n",
    "    \n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, \" won\")\n",
    "        else:\n",
    "            print(\"Draw\")\n",
    "        break\n",
    "        \n",
    "    player = tictactoe.get_opponent(player)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec6896a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
